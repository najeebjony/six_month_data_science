{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Piplines Using Transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import pipline from transformers\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#import pipline from transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [09:55<00:00, 450kB/s] \n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Downloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 9.62kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 683kB/s]\n"
     ]
    }
   ],
   "source": [
    "# create a genrator classifier pipeline\n",
    "classifier = pipeline(task=\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9995139837265015}]\n"
     ]
    }
   ],
   "source": [
    "text = \"i am going \"\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading config.json: 100%|██████████| 665/665 [00:00<00:00, 74.1kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 548M/548M [25:12<00:00, 362kB/s] \n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 5.21kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 364kB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:01<00:00, 375kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:03<00:00, 348kB/s]\n"
     ]
    }
   ],
   "source": [
    "genrator = pipeline(task=\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the world is a mess of things.' He added, 'the idea that we will not be able to be successful will not be accepted. This 'new normal' will be a place where no one can go and that there will be no progress at all. It is our very duty as people with a strong background in science to understand and accept this new normal.' The writer went on to suggest that for the next 20 years the world will be transformed into a 'place of pain', and as he\n"
     ]
    }
   ],
   "source": [
    "result = genrator(\"the world is a\", max_length=100)\n",
    "print(result[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
