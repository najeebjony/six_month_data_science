{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline in Machine Learning \n",
    " =============================\n",
    "This is a simple pipeline in machine learning. It is a simple example of how to use the pipeline in machine learning.\n",
    "This repository contains a collection of Jupyter notebooks that demonstrate various aspects of machine learning. The goal is to provide an interactive environment for expl\n",
    "oring machine learning ideas and techniques. The notebooks are written in Python 3 and include the scikit-learn library.\n",
    "The notebooks are loosely inspired by the book [Python Machine Learning](https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130) by [Sebastian Raschka](https://sebastianraschka.com/).\n",
    "The notebooks are intended to be used with the [Anaconda](https://www.continuum.io/downloads) Python distribution.\n",
    "\n",
    "Here are the key components of a pipeline:\n",
    "\n",
    "- **Data Preparation**: This step includes data cleaning, data transformation, and data reduction. Data cleaning is the process of removing or correcting the noisy data. Data transformation is the process of converting the data from one format to another format. Data reduction is the process of reducing the data size but still maintaining the integrity of the data.\n",
    "\n",
    "- **Model Evaluation**:\n",
    "This step includes model evaluation, model selection, and model tuning. Model evaluation is the process of evaluating the model performance. Model selection is the process of selecting the best model. Model tuning is the process of tuning the model parameters to improve the model performance.\n",
    "\n",
    "predictions:\n",
    "This step includes model deployment, model monitoring, and model explainability. Model deployment is the process of deploying the model to production. Model monitoring is the process of monitoring the model performance in production. Model explainability is the process of explaining the model predictions.\n",
    "\n",
    "# The main Advantage of using pipeline in machine learning are:\n",
    "- **Easy to read and understand**: The pipeline is easy to read and understand. This makes it easier for the reader to understand the pipeline.\n",
    "\n",
    "`simplified Workflow`: \n",
    "\n",
    "- **Data Preparation**: The data preparation step is simplified. The data preparation step is easy to understand and can be easily implemented.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- [Data Preparation](20_pipeline_machine_learning.ipynb)\n",
    "- [Feature Engineering](30_feature_engineering.ipynb)\n",
    "- [Feature Selection](40_feature_selection.ipynb)\n",
    "- [Model Evaluation](50_model_evaluation.ipynb)\n",
    "- [Model Tuning](60_model_tuning.ipynb)\n",
    "- [Model Optimization](70_model_optimization.ipynb)\n",
    "- [Model Deploying](80_model_deploying.ipynb)\n",
    "- [Model Monitoring](90_model_monitoring.ipynb)\n",
    "- [Model Explainability](100_model_explainability.ipynb)\n",
    "- [Model Management](110_model_management.ipynb)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import #  # # \n",
    "\n",
    "ww@@@@@#333333###CKCKFKKFKFKFKFK\n",
    "# load the titanic dataset from seaborn\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# select Features and target variable \n",
    "X = titanic[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic['survived']\n",
    "\n",
    "# split the data into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Define the column tansforme for imputing missing values \n",
    "categorical_features = ['pclass', 'sex', 'embarked']\n",
    "numerical_features = ['age', 'fare']\n",
    "\n",
    "# create the numerical transformer\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# create the categorical transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# create the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# create the column transformer\n",
    "Pipeline = Pipeline(steps=[\n",
    "    ('Preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))  # Change RandomForestRegressor to RandomForestClassifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data \n",
    "Pipeline.fit(X_train,y_train)\n",
    "\n",
    "# Make the predictions on the test data \n",
    "y_pred = Pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate and calculate the model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter Tunning in pipeline:\n",
    "- **Hyperparameter Tuning**: Hyperparameter tuning is the process of finding the best hyperparameters for a given model. Hyperparameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8212290502793296\n",
      "Best hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load the titanic dataset from seaborn\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# select Features and target variable\n",
    "X = titanic[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic['survived']\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline =Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "\n",
    "])\n",
    "\n",
    "#Define thw hyperparameter to tune \n",
    "# Define the hyperparameters to tune\n",
    "hyperparameters = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 5, 10],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "# perform grid search cross-validation\n",
    "grid_search = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# make prediction on the test data using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# print the best hyperparameter\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
