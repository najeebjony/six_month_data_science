{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Gegression:\n",
    "\n",
    "introdaction:\n",
    "    - Linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\n",
    "    - Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous.\n",
    "    - Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\n",
    "    - Linear regression has many practical uses. Most applications fall into one of the following two broad categories:\n",
    "        - If the goal is prediction, or forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\n",
    "        - Given a variable Y and a number of variables X1, ..., Xp that may be related to Y, linear regression analysis can be applied to quantify the strength of the relationship between Y and the Xj, to assess which Xj may have no relationship with Y at all, and to identify which subsets of the Xj contain redundant information about Y.\n",
    "    - Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely\n",
    "\n",
    "    - Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\n",
    "\n",
    "    - Linear regression has many practical uses. Most applications fall into one of the following two broad categories:\n",
    "\n",
    "        - If the goal is prediction, or forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\n",
    "\n",
    "        - Given a variable Y and a number of variables X1, ..., Xp that may be related to Y, linear regression analysis can be applied to quantify the strength of the relationship between Y and the Xj, to assess which Xj may have no relationship with Y at all, and to identify which subsets of the Xj contain redundant information about Y.\n",
    "\n",
    "\n",
    "    - Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely\n",
    "\n",
    "\n",
    "# Equvation of linear regression:\n",
    "\n",
    "$y = b0 + b1*x1 + b2*x2 + ... + bp*xp$\n",
    "\n",
    "## Example:\n",
    "- We have a dataset with two features x1 and x2, and one target variable y\n",
    "\n",
    "```plaintext\n",
    "x1    x2    y\n",
    "5.1   3.5   1.4\n",
    "4.9   3.0   1.4\n",
    "4.7   3.2   1.3\n",
    "4.6   3.1   1.5\n",
    "5.0   3.6   1.4\n",
    "```\n",
    "\n",
    "- We want to predict the target variable y using the two features x1 and x2\n",
    "\n",
    "\n",
    "## Solution:\n",
    "\n",
    "```plaintext\n",
    "y = b0 + b1*x1 + b2*x2\n",
    "```\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "- Linear regression can be used to predict the target variable y using the two features x1 and x2\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Linear_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Multi-linear Regression\n",
    "---\n",
    "## Introduction\n",
    "- Multi-linear regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome variable). The variables we are using to predict the value of the dependent variable are called the independent variables (or sometimes, the predictor variables).\n",
    "\n",
    "Multi-linear regression is a type of linear regression that allows for the inclusion of multiple independent variables in\n",
    "the model.\n",
    "\n",
    "# Mathematical Formulation:\n",
    "\n",
    "$y = b0 + b1*x1 + b2*x2 + ... + bp*xp$\n",
    "\n",
    "## Estimation of cofficients:\n",
    "\n",
    "- The regression coefficients, b0, b1, b2, ..., bp, are estimated using the Ordinary Least Squares (OLS) method.\n",
    "\n",
    "\n",
    "## Example:\n",
    "\n",
    "```plaintext\n",
    "x1    x2    y\n",
    "5.1   3.5   1.4\n",
    "4.9   3.0   1.4\n",
    "4.7   3.2   1.3\n",
    "4.6   3.1   1.5\n",
    "5.0   3.6   1.4\n",
    "```\n",
    "\n",
    "- We want to predict the target variable y using the two features x1 and x2\n",
    "\n",
    "\n",
    "## Solution:\n",
    "\n",
    "```plaintext\n",
    "y = b0 + b1*x1 + b2*x2\n",
    "```\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "- Multi-linear regression can be used to predict the target variable y using the two features x1 and x2\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Linear_regression\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
